# English-to-STL Dual Decoder Transformer

## ğŸ“ Introduction: English â†’ STL Translation

Signal Temporal Logic (STL) is widely used in robotics, autonomous systems, and cyber-physical systems to formally specify behaviors over time. Manually writing STL formulas is difficult, requires expertise, and is errorâ€‘prone.

This project provides an **automatic translation system** that converts **natural English instructions** into precise **STL formulas** using a specially designed **Dualâ€‘Decoder Transformer model**. The model separates semantic meaning and syntactic structure, ensuring accurate and grammatically valid STL outputs.

Below is the full architecture, file structure, usage instructions, and examples.

This repository implements an **English â†’ STL (Signal Temporal Logic)** translation system using a **Dual-Decoder Transformer** architecture. It contains all modules required for preprocessing, training, validation, testing, beam search decoding, and evaluation.

The model architecture, training flow, and performance results are summarized below along with instructions on how to run the code.

---

## ğŸ“Œ Architecture Overview

![Architecture](documents/dig.png)

**Key idea:** The model uses a shared encoder and two decoders:

* **Semantic Decoder** â†’ captures semantic meaning
* **Syntactic Decoder** â†’ captures STL grammar structure
* **Gated Fusion** blends both representations to generate final STL.

---

## ğŸ“‚ Project File Structure

```
NLP/
â”œâ”€â”€ data_preprocessing/
â”œâ”€â”€ subword/
â”‚   â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ tokenizer/
â”œâ”€â”€ public/
â”œâ”€â”€ str_process/
â”œâ”€â”€ test_cases/
â”œâ”€â”€ transformer/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ add_norm.py
â”‚   â”‚   â”œâ”€â”€ ffn.py
â”‚   â”‚   â”œâ”€â”€ transformer_encoder.py
â”‚   â”‚   â”œâ”€â”€ dual_transformer_decoder.py
â”‚   â”œâ”€â”€ test/
â”‚   â”‚   â”œâ”€â”€ predict_greedy.py
â”‚   â”‚   â”œâ”€â”€ transformer_predict_beam.py
â”‚   â”œâ”€â”€ train_validate/
â”‚   â”‚   â”œâ”€â”€ transformer_train_dev_data_iterator.py
â”œâ”€â”€ data_set/
â”œâ”€â”€ subword_preprocessing.py
â”œâ”€â”€ transformer_train.py
â”œâ”€â”€ transformer_test.py
â”œâ”€â”€ transformer_trainer_validator.py
â”œâ”€â”€ utils.py
```

---

## ğŸ”‘ Main Components Explained

### **1. transformer_train.py** (Main Training Script)

* Loads dataset
* Applies tokenization & batching
* Builds the Transformer + dual decoder
* Runs training loop
* Saves checkpoints

### **2. transformer_test.py** (Evaluate on Test Set)

* Loads saved model
* Runs evaluation on string-level and template-level STL
* Reports accuracy & BLEU score

### **3. transformer_predict_beam.py** (Beam Search Inference)

* Implements beam decoding
* Produces higher-quality STL outputs compared to greedy

### **4. transformer_hyperparas.py**

* Contains all tuning parameters
* Model size, layers, dropout, learning rate, fusion weights, etc.

### **5. transformer_train_dev_data_iterator.py**

* Handles batching
* Padding, masks, teacher forcing, etc.

### **6. transformer_encoder.py**

* Standard transformer encoder
* Multi-head self-attention + FFN + AddNorm

### **7. dual_transformer_decoder.py**

* Semantic decoder
* Syntactic decoder
* Gated fusion module

---

## â–¶ï¸ How to Run

### **1. Install Requirements**

```
pip install -r requirements.txt
```

### **2. Train the Model**

```
python transformer_train.py
```

### **3. Test the Model**

```
python transformer_test.py
```
---

## ğŸ“Š Experimental Results

![Results](documents/results.jpg)

**Summary:**

* **Train Accuracy:** 0.997 Â± 0.0064
* **Validation Accuracy:** 0.994 Â± 0.0031
* **Test String Accuracy:** 0.731 Â± 0.0131
* **Test Template Accuracy:** 0.893 Â± 0.0423
* **BLEU Score:** 0.963 Â± 0.0231

---

## ğŸ’¡ English â†’ STL Examples

### **Example 1**

**Input:**

```
The robot must reach region A within 10 seconds.
```

**Output STL:**

```
F[0,10] (robot_in_A)
```

### **Example 2**

**Input:**

```
The temperature should always stay below 80.
```

**Output STL:**

```
G (temp < 80)
```

### **Example 3**

**Input:**

```
If obstacle appears, eventually stop.
```

**Output STL:**

```
(obstacle) â†’ F(stop)
```

---

## âœ”ï¸ Summary

This repository provides a complete implementation of an English â†’ STL generator using a dual-decoder transformer. The system achieves high accuracy and BLEU scores and supports flexible inference via greedy or beam search.

Feel free to extend the model, add datasets, or integrate with your downstream robotics/verification tasks.
